<!DOCTYPE html>
<html lang="en-us">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>pdebashis/posts/hadoop/</title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="robots" content="all,follow">
    <meta name="googlebot" content="index,follow,snippet,archive">
    <link rel="stylesheet" href="https://pdebashis.github.io/hugo-theme-console/css/terminal-0.7.2.min.css">
    <link rel="stylesheet" href="https://pdebashis.github.io/hugo-theme-console/css/animate-4.1.1.min.css">
    <link rel="stylesheet" href="https://pdebashis.github.io/hugo-theme-console/css/console.css">
    
      <!--[if lt IE 9]>
          <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
          <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
      <![endif]-->
       <meta property="og:title" content="Monitoring Hadoop" />
<meta property="og:description" content="" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://pdebashis.github.io/posts/hadoop/" /><meta property="article:published_time" content="2021-06-12T17:34:37+05:30" />



<meta name="twitter:title" content="Monitoring Hadoop"/>
<meta name="twitter:description" content="A Hadoop ecosystem requires vigorous monitoring to identify any abnormality and prevent major downtimes. Major Hadoop service providers, provide their own tools for monitoring. But for a standalone hadoop, the monitoring aspect is an open field, and you can use your own tools and scripts to setup a monitoring framework.
Here, lets try to setup a basic script based monitoring capability with essential indicators
Uptime Monitoring Identify each process type running on each cluster."/>

</head>
<body class="terminal">
    <div class="container">
        <div class="terminal-nav">
          <header class="terminal-logo">
            <div class="logo terminal-prompt">
              
              
              <a href="https://pdebashis.github.io/" class="no-style site-name">pdebashis</a>:~# 
              <a href='https://pdebashis.github.io/posts'>posts</a>/<a href='https://pdebashis.github.io/posts/hadoop'>hadoop</a>/</div></header>
          <nav class="terminal-menu">
            <ul vocab="https://schema.org/" typeof="BreadcrumbList">
                
                <li><a href="https://pdebashis.github.io/about/" typeof="ListItem">about/</a></li>
                
                <li><a href="https://pdebashis.github.io/posts/" typeof="ListItem">blog/</a></li>
                
                <li><a href="https://pdebashis.github.io/photos/" typeof="ListItem">photos/</a></li>
                
                <li><a href="https://pdebashis.gitlab.io/my-dashboard" typeof="ListItem">dashboard/</a></li>
                
            </ul>
          </nav>
        </div>
    </div>

    <div class="container animated zoomIn fast" >
        
<h1>Monitoring Hadoop</h1>

Jun. 12, 2021


<br/><br/>
<p>A Hadoop ecosystem requires vigorous monitoring to identify any abnormality and prevent major downtimes. Major Hadoop service providers, provide their own tools for monitoring. But for a standalone hadoop, the monitoring aspect is an open field, and you can use your own tools and scripts to setup a monitoring framework.</p>
<p>Here, lets try to setup a basic script based monitoring capability with essential indicators</p>
<h2 id="uptime-monitoring">Uptime Monitoring</h2>
<p>Identify each process type running on each cluster. For a more detailed output, read the uptime from ps output.</p>
<pre><code>PID1=`pgrep -f &quot;proc_datanode&quot;`
PID2=`pgrep -f &quot;\&lt;namenode\&gt;&quot;`
PID3=`pgrep -f &quot;\&lt;JobHistoryServer\&gt;&quot;`
PID4=`pgrep -f &quot;\&lt;DFSZKFailoverController\&gt;&quot;`
PID5=`pgrep -f &quot;\&lt;NodeManager\&gt;&quot;`
PID6=`pgrep -f &quot;\&lt;ResourceManager\&gt;&quot;`
PID7=`pgrep -f &quot;\&lt;JournalNode\&gt;&quot;`
</code></pre>
<p>For each PID, the check will look something like this -</p>
<pre><code>if [ -z $PID ]
then
    IS_ALL_OK=1
    printf &quot;%15s [ %5s ]\n&quot; &quot;Data Node&quot; &quot;ALERT&quot;
else
    printf &quot;%15s [ %5s ]\n&quot; &quot;Data Node&quot; &quot;OK&quot;
fi
</code></pre>
<h2 id="cluster-status">Cluster Status</h2>
<p>Namenode status</p>
<pre><code>hdfs haadmin -getAllServiceState
</code></pre>
<p>Resouce Manager status</p>
<pre><code>yarn rmadmin -getAllServiceState
</code></pre>
<h2 id="resource-utilization">Resource Utilization</h2>
<p>HDFS disk utilisations</p>
<pre><code>hdfs dfs -df -h
</code></pre>
<p>Live data nodes</p>
<pre><code>grab_report=`$HADOOP_PREFIX/bin/hdfs dfsadmin -report 2&gt;/dev/null`
hosts=$(echo -n &quot;$grab_report&quot; | grep Name)
while IFS= read -r i;do
  host=$(echo $i | awk -F'[()]' '{print $2}')
  totaldisk=`echo -n &quot;$grab_report&quot; | grep &quot;$i&quot; -A 7 | grep &quot;Configured Capacity:&quot; | awk -F'[()]' '{print $2}'`
  useddisk=`echo -n &quot;$grab_report&quot; | grep &quot;$i&quot; -A 7 | grep &quot;^DFS Used:&quot; | awk -F'[()]' '{print $2}'`
  availdisk=`echo -n &quot;$grab_report&quot; | grep &quot;$i&quot; -A 7 | grep &quot;DFS Remaining:&quot; | awk -F'[()]' '{print $2}'`
  perc=`echo -n &quot;$grab_report&quot; | grep &quot;$i&quot; -A 7 | grep &quot;DFS Used%:&quot; | awk -F':' '{print $2}'`
echo &quot;DATANODE,$host,$totaldisk,$useddisk,$availdisk,$percdisk&quot;
done &lt;&lt;&lt; &quot;$hosts&quot;
</code></pre>
<p>Yarn memory usage</p>
<pre><code>http:/hostname:8088
</code></pre>
<p>Network Usage</p>
<pre><code>tcpdump,nmap,wireshark amongst others
</code></pre>
<p>CPU &amp; Memory Usage</p>
<pre><code>top
</code></pre>
<h2 id="hbase-monitoring">Hbase Monitoring</h2>
<p>Regions Count</p>
<pre><code>hmaster=$(curl -s http://$hbase_IP:16010/jmx)
liveserver=$(echo -n &quot;$hmaster&quot; | grep &quot;tag.liveRegionServers&quot; | awk -F':' '{print $2}' | tr '&quot;' ' ' )
IFS=';' read -r -a array &lt;&lt;&lt; &quot;$liveserver&quot;

for i in ${array[@]}; do
  rserver=$(echo $i | cut -d ',' -f1)
  if [ ! -z $rserver ]; then
  region_count=$(curl -s http://$rserver:16030/jmx | grep regionCount | cut -d':' -f2)
  echo &quot;REGIONS,$rserver,$region_count&quot; &gt;&gt; $outputfile
  fi
done
</code></pre>
<p>Regions Skew</p>
<pre><code>grab_hbase_tables=$($HADOOP_PREFIX/bin/hdfs dfs -du -h  /hbase2/data/default)
for table in ${tables[@]}; do
  size=$(echo -n &quot;${grab_hbase_tables}&quot; | grep /${table}$ | awk '{print $1$2}')
  regions_table=$(curl -s -N &quot;http://${hbase_IP}:16010/table.jsp?name=$table&quot; | readRegions | grep &quot;^&lt;td&gt;&quot; | grep -v href | grep -o -E '[0-9]+')
  regionscount=$(echo &quot;${regions_table}&quot; | awk '{sum+=$0}END{print sum}')
  regionsall=$(echo &quot;${regions_table}&quot; | paste -s -d/ -)
  echo &quot;HBASE,$table,$size,$regionscount,$regionsall&quot; &gt;&gt; $outputfile
done
</code></pre>
<h2 id="inconsistencies">Inconsistencies</h2>
<p>HDFS block inconsistencies</p>
<pre><code>hdfs fsck / | egrep -v '^\.+$' | grep -v eplica 
</code></pre>
<p>Hbase block inconsistencies</p>
<pre><code>hbase hbck -details
</code></pre>
<h2 id="conclusion">Conclusion</h2>
<p>All the above and similar indicators can be run on the cluster in a script to get scheduled reports in mail. You could also build grafana dashboards using most of the indicators.</p>



        <div class="footer">
    Powered by <a href="https://gohugo.io/">Hugo</a> with
    <a href="https://github.com/mrmierzejewski/hugo-theme-console/">Console Theme</a>. 
</div>

    </div>
  </body>
</html>
